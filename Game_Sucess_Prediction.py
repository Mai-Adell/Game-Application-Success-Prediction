# -*- coding: utf-8 -*-
"""Finallllll.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tdwAYHpZRRIJdqM0NL-usMpfsPhHKWCj
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
import datetime as datee
from sklearn import metrics
from sklearn.metrics import r2_score
from sklearn import linear_model
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import Ridge
import itertools
import functools
import regex as re
import statistics
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.naive_bayes import GaussianNB
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from pickle import dump
from pickle import load

def geners2(xtrain,xtest):
    geners_dict= {}
    xtrain_list = xtrain.str.split(', ')
    xtrain_array = np.array(xtrain_list)
    xtest_list = xtest.str.split(', ')
    xtest_array = np.array(xtest_list)

    # count the languages to get the max
    for generslist in xtrain_array:
        for gern in generslist:
            if gern not in geners_dict:
                geners_dict[gern] = 1
            else:
                geners_dict[gern] += 1
    for generslist in xtest_array:
        for gern in generslist:
            if gern not in geners_dict:
                geners_dict[gern] = 1
            else:
                geners_dict[gern] += 1
    #all rows contain games so delete it
    geners_dict['Games']=0

    ltrain=[]
    for i in xtrain_list:
        sum=0
        for j in i:
            sum+=geners_dict[j]
        ltrain.append(sum)

    xtrain=np.array(ltrain)
    ltest=[]
    for i in xtest_list:
        sum=0
        for j in i:
            sum+=geners_dict[j]
        ltest.append(sum)

    xtest=np.array(ltest)
    return xtrain,xtest

def language2(x_train , x_test):

    x_train['Languages'].fillna("No language", inplace = True)
    x_test['Languages'].fillna("No language", inplace = True)

    #language column
    language_dict= {}
    Language_data1 = x_train['Languages'].str.split(', ')
    Language_array = np.array(Language_data1)

    # count the languages to get the max
    for langlist in Language_array:
        for lang in langlist:
            if lang not in language_dict:
             language_dict[lang] = 1
            else:
             language_dict[lang] += 1


    Language_data2 = x_test['Languages'].str.split(', ')
    Language_array = np.array(Language_data2)

    # count the languages to get the max
    for langlist in Language_array:
        for lang in langlist:
            if lang not in language_dict:
             language_dict[lang] = 1
            else:
             language_dict[lang] += 1

    s1 = []
    i = 0
    for langlist in Language_data1 :
        counter = 0
        for lang in langlist:
          if lang == "No language":
           counter = statistics.median(language_dict.values())
           continue
          counter = counter + language_dict[lang]
        s1.append(counter)
        i = i+1

    s2 = []
    i = 0
    for langlist in Language_data2 :
       counter = 0
       for lang in langlist:
        if lang == "No language":
          counter = statistics.median(language_dict.values())
          continue
        counter = counter + language_dict[lang]
       s2.append(counter)
       i = i+1

    x_train['Languages'] = s1
    x_test['Languages'] = s2


    return x_train , x_test

def ANOVA_sel(x_train , y_train):
  selector = SelectKBest(f_classif, k=4)
  selector.fit(x_train, y_train)
  cols = selector.get_support(indices=True)
  return cols

def corrr (column):
   top3=[]
   for u in range(5):
        max=-2
        for i in column :
            if(i>=max):
                max=i

        top3.append(max)

        column= column.replace(max,-2)
   return top3[4]

def In_app(data):

    data['In-app Purchases'].fillna( "-1.0" , inplace = True)
    inapp_data = data['In-app Purchases'].str.split(', ')
    inapp_data
    ss= []
    sss= []
    for inapplist in inapp_data:
        uniq = []
        counter = 0.0
        for inapp in inapplist:
            uniq.append(inapp)

        uniqu = set(uniq)
        uniqlst = list(uniqu)
        for i in uniqlst:
            counter = counter + np.float16(i)
        if counter != -1.0 :
            ss.append(round(counter,2))
        sss.append(round(counter,2))


    for i in range(0 , len(sss)):
        if sss[i] == -1.0:
                 sss[i] = statistics.median(ss)


    data['In-app Purchases'] = sss
    return data

def removeoutlayers(data):
    for i in data :
      data=data[np.abs(data[i]-data[i].mean()) <= (3*data[i].std())]
    return data

def preprocessing (X_train,X_test,y_train,y_test , y_name):

        # X_train
        X_train.drop(['URL'],axis=1,inplace=True)
       # X_train=Genres(X_train)
        train=X_train
        train[y_name]=y_train
        train=train.drop_duplicates()
        y_train=train[y_name]
        X_train=train.drop([y_name],axis=1)
        X_train,X_test=language2(X_train,X_test)
        print(X_train.dtypes)
        X_train=X_train.replace(to_replace="https://is[0-9]-ssl.mzstatic.com/image/thumb/Purple",value="",regex=True)
        X_train=X_train.replace(to_replace="/source/512x512bb.jpg",value="",regex=True)
        X_train['Original Release Date']=pd.to_datetime(X_train['Original Release Date'],dayfirst=True)
        X_train['Original Release Date']=X_train['Original Release Date'].map(datee.datetime.toordinal)
        X_train['Current Version Release Date']=pd.to_datetime(X_train ['Current Version Release Date'],dayfirst=True)
        X_train['Current Version Release Date']=X_train['Current Version Release Date'].map(datee.datetime.toordinal)
        X_train['Age Rating'] = X_train['Age Rating'].str.replace('+', '')
        X_train['Age Rating'] = X_train['Age Rating'].apply(lambda x : int(x))
        X_train= In_app(X_train)

        # X_test
        X_test.drop(['URL'],axis=1,inplace=True)
      #  X_test=Genres(X_test)

        X_train['Genres'],X_test['Genres']=geners2( X_train['Genres'],X_test['Genres'])
        X_test=X_test.replace(to_replace="https://is[0-9]-ssl.mzstatic.com/image/thumb/Purple",value="",regex=True)
        X_test=X_test.replace(to_replace="/source/512x512bb.jpg",value="",regex=True)
        X_test['Original Release Date']=pd.to_datetime(X_test['Original Release Date'],dayfirst=True)
        X_test['Original Release Date']=X_test['Original Release Date'].map(datee.datetime.toordinal)
        X_test['Current Version Release Date']=pd.to_datetime(X_test['Current Version Release Date'],dayfirst=True)
        X_test['Current Version Release Date']=X_test['Current Version Release Date'].map(datee.datetime.toordinal)
        X_test['Age Rating'] = X_test['Age Rating'].str.replace('+', '')
        X_test['Age Rating'] = X_test['Age Rating'].apply(lambda x : int(x))
        X_test= In_app(X_test)



        xtrainandtest=X_train.append(X_test)
        ytrainandtest=y_train.append(y_test)

        # Encoderr1 = load(open('Encoder1.pkl', 'rb'))

        Encoderr1 =LabelEncoder()
        fitting=Encoderr1.fit(xtrainandtest['Name'])
        X_train['Name']=Encoderr1.transform(X_train['Name'])
        X_test['Name']=Encoderr1.transform(X_test['Name'])

        dump(Encoderr1, open('Encoder1.pkl', 'wb'))

        # Encoderr2 = load(open('Encoder2.pkl', 'rb'))

        Encoderr2 =LabelEncoder()
        fitting=Encoderr2.fit(xtrainandtest['Subtitle'])
        X_train['Subtitle']=Encoderr2.transform(X_train['Subtitle'])
        X_test['Subtitle']=Encoderr2.transform(X_test['Subtitle'])

        dump(Encoderr2, open('Encoder2.pkl', 'wb'))

        # Encoderr3 = load(open('Encoder3.pkl', 'rb'))

        Encoderr3 =LabelEncoder()
        fitting=Encoderr3.fit(xtrainandtest['Icon URL'])
        X_train['Icon URL']=Encoderr3.transform(X_train['Icon URL'])
        X_test['Icon URL']=Encoderr3.transform(X_test['Icon URL'])

        dump(Encoderr3, open('Encoder3.pkl', 'wb'))

        # Encoderr4 = load(open('Encoder4.pkl', 'rb'))

        Encoderr4 =LabelEncoder()
        fitting=Encoderr4.fit(xtrainandtest['Developer'])
        X_train['Developer']=Encoderr4.transform(X_train['Developer'])
        X_test['Developer']=Encoderr4.transform(X_test['Developer'])

        dump(Encoderr4, open('Encoder4.pkl', 'wb'))


        # Encoderr5 = load(open('Encoder5.pkl', 'rb'))

        Encoderr5 =LabelEncoder()
        fitting=Encoderr5.fit(xtrainandtest['Primary Genre'])
        X_train['Primary Genre']=Encoderr5.transform(X_train['Primary Genre'])
        X_test['Primary Genre']=Encoderr5.transform(X_test['Primary Genre'])

        dump(Encoderr5, open('Encoder5.pkl', 'wb'))


        # X_train_median = load(open('X_train_median.pkl', 'rb'))
        # y_train_median = load(open('y_train_median.pkl', 'rb'))

        X_train_median = X_train.median()
        y_train_median = y_train.median()
        X_train = X_train.fillna(X_train_median)
        X_test = X_test.fillna(X_train_median)
        y_train = y_train.fillna(y_train_median)
        y_test = y_test.fillna(y_train_median)


        train=X_train
        train[y_name]=y_train

        dump(X_train_median, open('Xtrain_median.pkl', 'wb'))
        dump(y_train_median, open('ytrain_median.pkl', 'wb'))

        if y_name == 'Average User Rating' :
          #feature selection by CORR
          # selected_feature = load( open('selected_features.pkl', 'rb'))
          corr=train.corr()
          selected_feature=corr.index[abs(corr[y_name])>=corrr(abs(corr[y_name]))]
          selected_feature=selected_feature.delete(-1)
          X_train=train[selected_feature]
          X_test=X_test[selected_feature]

          dump(selected_feature, open('selected_feature.pkl', 'wb'))

        train=X_train
        train[y_name]=y_train
        train=removeoutlayers(train)
        train=train.drop_duplicates()
        y_train=train[y_name]
        X_train=train.drop([y_name],axis=1)

        test=X_test
        test[y_name]=y_test
        test=removeoutlayers(test)
        test=test.drop_duplicates()
        y_test=test[y_name]
        X_test=test.drop([y_name],axis=1)

        if y_name == 'Average User Rating' :

          # scaler1 = load(open('scalerr1.pkl', 'rb'))

          scaler1=MinMaxScaler()
          scaler1.fit(X_train)
          X_train=pd.DataFrame(scaler1.transform(X_train),columns=X_train.columns)
          X_test=pd.DataFrame(scaler1.transform(X_test),columns=X_test.columns)
          dump(scaler1, open('scalerr1.pkl', 'wb'))

        elif y_name == 'Rate' :
          # scaler2 = load(open('scalerr2.pkl', 'rb'))
          scaler2=MinMaxScaler()
          scaler2.fit(X_train)
          X_train=pd.DataFrame(scaler2.transform(X_train),columns=X_train.columns)
          X_test=pd.DataFrame(scaler2.transform(X_test),columns=X_test.columns)
          dump(scaler2, open('scalerr2.pkl', 'wb'))


        return X_train,X_test,y_train,y_test

# selected_feature = load( open('selected_features.pkl', 'rb'))
# print(selected_feature)

#Regression
regression_data = pd.read_csv('games-regression-dataset.csv')
y_name = 'Average User Rating'
regression_data.drop(['Description'],axis=1,inplace=True)

x=regression_data
x=x.drop([y_name],axis=1)
y=regression_data[y_name]

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3,shuffle=False)
#preprocessing
X_train_regression,X_test_regression,y_train_regression,y_test_regression=preprocessing (X_train,X_test,y_train,y_test , y_name)

if y_name == 'Average User Rating':
  #Models
  #lLinear Regression
  # model = load(open('linear_regression.pkl', 'rb'))
  model = linear_model.LinearRegression()
  model.fit(X_train_regression , y_train_regression)
  pre1 = model.predict(X_test_regression)
  R_square2 = model.score(X_test_regression , y_test_regression)
  print('MSE : ' , metrics.mean_squared_error(np.asarray(y_test_regression) ,pre1))
  print('R square : ' ,R_square2*100 )

  dump(model, open('Linear_regression.pkl', 'wb'))

  plt.scatter(y_test_regression, pre1 , c='crimson')
  p1 = max(max(pre1 ), max(y_test_regression))
  p2 = min(min(pre1 ), min(y_test_regression))
  plt.plot([p1, p2], [p1, p2], 'b-')
  plt.xlabel('True Values', fontsize=15)
  plt.ylabel('Predictions', fontsize=15)
  plt.show()

#Polynomial Regression
# poly = load(open('polynomial_features.pkl', 'rb'))
poly = PolynomialFeatures(degree=2)
x_train_poly = poly.fit_transform(X_train_regression)
x_test_poly = poly.transform(X_test_regression)
# model = load(open('polynomial_regression.pkl', 'rb'))
model = linear_model.LinearRegression()
model.fit(x_train_poly , y_train_regression)
pre = model.predict(x_test_poly)
R_square1 = model.score(x_test_poly,y_test_regression)
print('MSE : ' , metrics.mean_squared_error(np.asarray(y_test_regression) ,pre))
print('R square : ' ,R_square1 )

dump(poly, open('Polynomial_features.pkl', 'wb'))
dump(model, open('Polynomial_regression.pkl', 'wb'))

plt.scatter(y_test_regression, pre , c='crimson')
p1 = max(max(pre ), max(y_test_regression))
p2 = min(min(pre ), min(y_test_regression))
plt.plot([p1, p2], [p1, p2], 'b-')
plt.xlabel('True Values', fontsize=15)
plt.ylabel('Predictions', fontsize=15)
plt.show()

#Ridge
# model=load(open('ridge.pkl', 'rb'))
model=Ridge(alpha=0.00001)
model=model.fit(X_train_regression,y_train_regression)
pred3=model.predict(X_test_regression)
print("test mean square error Ridge:",metrics.mean_squared_error(y_test_regression,pred3))
print("r2-score of Ridge :",r2_score(y_test_regression,pred3))

dump(model, open('Ridge.pkl', 'wb'))

plt.scatter(y_test_regression, pred3 , c='crimson')
p1 = max(max(pred3 ), max(y_test_regression))
p2 = min(min(pred3 ), min(y_test_regression))
plt.plot([p1, p2], [p1, p2], 'b-')
plt.xlabel('True Values', fontsize=15)
plt.ylabel('Predictions', fontsize=15)
plt.show()

#Classification
classification_data = pd.read_csv('games-classification-dataset.csv')
y_name = 'Rate'
classification_data.drop(['Description'],axis=1,inplace=True)

x=classification_data
x=x.drop([y_name],axis=1)
y=classification_data[y_name]
cleanup_nums = {'Rate':{"Low": 1, "Intermediate": 2 , "High" : 3}}
classification_data = classification_data.replace(cleanup_nums)
y=classification_data['Rate']

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3,shuffle=False)

#preprocessing
X_train_classification,X_test_classification,y_train_classification,y_test_classification=preprocessing (X_train,X_test,y_train,y_test , y_name)

if y_name == 'Rate':
  # cols=load(open('anova.pkl', 'rb'))
  # Feature selection by ANOVA
  cols = ANOVA_sel(X_train_classification , y_train_classification)
  x_train_classification = X_train_classification.iloc[:,cols]
  x_test_classification = X_test_classification.iloc[:,cols]

  dump(cols, open('anovaa.pkl', 'wb'))

# cols=load(open('anova.pkl', 'rb'))
# print(cols)

#Models
  #Nieve base
  # gnb = load( open('naive_bayes.pkl', 'rb'))
  gnb = GaussianNB()
  gnb.fit(x_train_classification, y_train_classification)
  y_pred = gnb.predict(x_test_classification)
  print(gnb.score(x_test_classification ,  y_test_classification))
  accuray = accuracy_score(y_pred, y_test_classification)
  print("Accuracy of Naive Bayes:",accuray)

  dump(gnb, open('Naive_bayes.pkl', 'wb'))

#Logistic Regression
# log= load(open('logistic_regression.pkl', 'rb'))
log=LogisticRegression(random_state=16)
log.fit(x_train_classification,y_train_classification)
x_test_pred=log.predict(x_test_classification)
test_accuracy=accuracy_score(x_test_pred,y_test_classification)
print("accuracy of testing data by Logistic Regression is : ",test_accuracy)

dump(log, open('Logistic_regression.pkl', 'wb'))

# Confusion Matrix
cn_matrix=metrics.confusion_matrix(y_test_classification,x_test_pred)
cn_matrix
cm_df = pd.DataFrame(cn_matrix,
                      index = ['Low ','Intermediate','High'],
                      columns = ['Low','Intermediate','High'])

#Plotting the confusion matrix
plt.figure(figsize=(5,4))
sns.heatmap(cm_df, annot=True)
plt.title('Confusion Matrix')
plt.ylabel('Actal Values')
plt.xlabel('Predicted Values')
plt.show()

#SVM
params = {
      'C': [0.1, 1, 10],
      'kernel': ['linear', 'rbf', 'poly'],
      'gamma': ['scale', 'auto']
  }
# grid_search_svm = load(open('svm.pkl', 'rb'))
svm = SVC()
grid_search_svm = GridSearchCV(estimator=svm, param_grid=params, cv=5, n_jobs=-1)
grid_search_svm.fit(X_train_classification, y_train_classification)
#print(grid_search_svm.best_params_)
x_test_pred = grid_search_svm.predict(X_test_classification)
test_accuracy=accuracy_score(x_test_pred,y_test_classification)
print("accuracy of testing data by SVM is : ",test_accuracy)

dump(grid_search_svm, open('SVM.pkl', 'wb'))

params = {
      'criterion': ['gini', 'entropy'],
      'max_depth': [None, 5, 10],
      'min_samples_split': [2, 5],
      'min_samples_leaf': [1, 2]
  }

# grid_search_dt = load(open('decision_tree.pkl', 'rb'))
dt = DecisionTreeClassifier()
grid_search_dt = GridSearchCV(estimator=dt, param_grid=params, cv=5, n_jobs=-1)
grid_search_dt.fit(X_train_classification, y_train_classification)
#print(grid_search_dt.best_params_)
x_test_pred = grid_search_dt.predict(X_test_classification)
test_accuracy=accuracy_score(x_test_pred,y_test_classification)
print("accuracy of testing data by Decision tree is : ",test_accuracy)

dump(grid_search_dt, open('Decision_tree.pkl', 'wb'))

# GradientBoostingClassifier model
from sklearn.ensemble import GradientBoostingClassifier

# gbm = load(open('gradient_boosting.pkl', 'rb'))
gbm = GradientBoostingClassifier()
gbm.fit(X_train_classification, y_train_classification)
# print(X_test_classification.shape)
#X_test_classification = X_test_classification.reshape(-1,1)
# x_test_pred = gbm.predict(X_test_classification)
accuracy_gbm = gbm.score(X_test_classification,y_test_classification)
print(f"GBM Accuracy: {accuracy_gbm*100}")

dump(gbm, open('Gradient_boosting.pkl', 'wb'))

#Test Script for regression
path ='games-regression-dataset.csv'
data = pd.read_csv(path)
y_name = 'Average User Rating'

f = data.isna().sum()

if data.isnull().any().any():
    X_train_median = load(open('Xtrain_median.pkl', 'rb'))
    y_train_median = load(open('ytrain_median.pkl', 'rb'))


X_test=data
X_test=X_test.drop([y_name],axis=1)
y_test=data[y_name]

selected_feature = ['ID', 'Subtitle', 'Original Release Date','Current Version Release Date']
X_test=X_test[selected_feature]


Encoderr2 = load(open('Encoder2.pkl', 'rb'))
X_test['Subtitle']=Encoderr2.transform(X_test['Subtitle'])

X_test = X_test.fillna(X_train_median)
y_test = y_test.fillna(y_train_median)


X_test['Original Release Date']=pd.to_datetime(X_test['Original Release Date'],dayfirst=True)
X_test['Original Release Date']=X_test['Original Release Date'].map(datee.datetime.toordinal)
X_test['Current Version Release Date']=pd.to_datetime(X_test ['Current Version Release Date'],dayfirst=True)
X_test['Current Version Release Date']=X_test['Current Version Release Date'].map(datee.datetime.toordinal)



scaler1 = load(open('scalerr1.pkl', 'rb'))
X_test=pd.DataFrame(scaler1.transform(X_test),columns=X_test.columns)

#lLinear Regression
model = load(open('Linear_regression.pkl', 'rb'))
pre1 = model.predict(X_test)
R_square2 = model.score(X_test , y_test)
print('MSE : ' , metrics.mean_squared_error(np.asarray(y_test) ,pre1))
print('R square : ' ,R_square2*100 )


#Polynomial Regression
poly = load(open('Polynomial_features.pkl', 'rb'))
x_test_poly = poly.transform(X_test)
model = load(open('Polynomial_regression.pkl', 'rb'))
pre = model.predict(x_test_poly)
R_square1 = model.score(x_test_poly,y_test)
print('MSE : ' , metrics.mean_squared_error(np.asarray(y_test) ,pre))
print('R square : ' ,R_square1 )


#Ridge
model=load(open('Ridge.pkl', 'rb'))
pred3=model.predict(X_test)
print("test mean square error Ridge:",metrics.mean_squared_error(y_test,pred3))
print("r2-score of Ridge :",r2_score(y_test,pred3))

#Test Script for classification
path ='games-classification-dataset.csv'
data = pd.read_csv(path)
y_name = 'Rate'

cleanup_nums = {'Rate':{"Low": 1, "Intermediate": 2 , "High" : 3}}
data = data.replace(cleanup_nums)

if data.isnull().any().any():
    X_train_median = load(open('Xtrain_median.pkl', 'rb'))
    y_train_median = load(open('ytrain_median.pkl', 'rb'))


cols=load(open('anovaa.pkl', 'rb'))

X_test=data
X_test=X_test.drop([y_name],axis=1)
y_test=data[y_name]

selected_feature = ['ID', 'Subtitle', 'Original Release Date','Current Version Release Date']
X_test=X_test[selected_feature]


Encoderr2 = load(open('Encoder2.pkl', 'rb'))
X_test['Subtitle']=Encoderr2.transform(X_test['Subtitle'])

X_test['Original Release Date']=pd.to_datetime(X_test['Original Release Date'],dayfirst=True)
X_test['Original Release Date']=X_test['Original Release Date'].map(datee.datetime.toordinal)
X_test['Current Version Release Date']=pd.to_datetime(X_test ['Current Version Release Date'],dayfirst=True)
X_test['Current Version Release Date']=X_test['Current Version Release Date'].map(datee.datetime.toordinal)


X_test = X_test.fillna(X_train_median)
y_test = y_test.fillna(y_train_median)


scaler1 = load(open('scalerr1.pkl', 'rb'))
X_test=pd.DataFrame(scaler1.transform(X_test),columns=X_test.columns)

#Naive Bayes
gnb = load( open('Naive_bayes.pkl', 'rb'))
y_pred = gnb.predict(X_test)
accuray = accuracy_score(y_pred, y_test)
print("Accuracy of Naive Bayes:",accuray)


#Logistic regression
log= load(open('Logistic_regression.pkl', 'rb'))
x_test_pred=log.predict(X_test)
test_accuracy=accuracy_score(x_test_pred,y_test)
print("accuracy of testing data by Logistic Regression is : ",test_accuracy)


#SVM
grid_search_svm = load(open('SVM.pkl', 'rb'))
test_accuracy=accuracy_score(x_test_pred,y)
print("accuracy of testing data by SVM is : ",test_accuracy)

#Decision tree
grid_search_dt = load(open('Decision_tree.pkl', 'rb'))
test_accuracy=accuracy_score(x_test_pred,y_test)
print("accuracy of testing data by Decision tree is : ",test_accuracy)


# GradientBoostingClassifier model
gbm = load(open('Gradient_boosting.pkl', 'rb'))
# x_test_pred = gbm.predict(X_test)
accuracy_gbm = gbm.score(X_test, y_test)
print(f"GBM Accuracy: {accuracy_gbm*100}")